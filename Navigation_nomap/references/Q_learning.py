import numpy as np
import random

# ç¯å¢ƒå‚æ•°
GRID_SIZE = 5  # ç½‘æ ¼å¤§å° (5x5)
START_POS = (0, 0)  # èµ·ç‚¹åæ ‡
PACKAGE_POS = (2, 2)  # åŒ…è£¹ä½ç½®
TARGET_POS = (4, 4)  # ç›®æ ‡ä½ç½®
ACTIONS = ['up', 'down', 'left', 'right']  # åŠ¨ä½œç©ºé—´

# Q-learningå‚æ•°
EPISODES = 1000  # è®­ç»ƒè½®æ•°
ALPHA = 0.1  # å­¦ä¹ ç‡
GAMMA = 0.99  # æŠ˜æ‰£å› å­
EPSILON = 0.1  # æ¢ç´¢æ¦‚ç‡


class CourierEnv:
    """é‚®å·®ä»»åŠ¡ç½‘æ ¼ç¯å¢ƒ"""

    def __init__(self):
        self.agent_pos = START_POS
        self.has_package = False
        self.done = False

    def reset(self):
        """é‡ç½®ç¯å¢ƒçŠ¶æ€"""
        self.agent_pos = START_POS
        self.has_package = False
        self.done = False
        return self._get_state()

    def _get_state(self):
        """è¿”å›å½“å‰çŠ¶æ€ï¼ˆåæ ‡ + æ˜¯å¦æºå¸¦åŒ…è£¹ï¼‰"""
        return (*self.agent_pos, int(self.has_package))

    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›æ–°çŠ¶æ€ã€å¥–åŠ±ã€æ˜¯å¦ç»ˆæ­¢"""
        x, y = self.agent_pos
        reward = -0.1  # æ—¶é—´æƒ©ç½šï¼ˆæ¯æ­¥-0.1ï¼‰

        # æ‰§è¡ŒåŠ¨ä½œ
        if action == 'up' and x > 0:
            x -= 1
        elif action == 'down' and x < GRID_SIZE - 1:
            x += 1
        elif action == 'left' and y > 0:
            y -= 1
        elif action == 'right' and y < GRID_SIZE - 1:
            y += 1
        else:
            reward = -1  # æ— æ•ˆåŠ¨ä½œæƒ©ç½š

        self.agent_pos = (x, y)

        # æ£€æŸ¥åŒ…è£¹æ‹¾å–
        if not self.has_package and self.agent_pos == PACKAGE_POS:
            self.has_package = True
            reward = 10  # æ‹¾å–åŒ…è£¹å¥–åŠ±

        # æ£€æŸ¥é€è¾¾ç›®æ ‡
        if self.has_package and self.agent_pos == TARGET_POS:
            self.done = True
            reward = 50  # æˆåŠŸé€è¾¾å¥–åŠ±

        return self._get_state(), reward, self.done


class QLearningAgent:
    """Q-learningæ™ºèƒ½ä½“"""

    def __init__(self):
        # åˆå§‹åŒ–Qè¡¨ï¼šçŠ¶æ€ç©ºé—´ä¸º (x, y, has_package), åŠ¨ä½œç©ºé—´ä¸º4
        self.q_table = np.zeros((GRID_SIZE, GRID_SIZE, 2, len(ACTIONS)))

    def choose_action(self, state, epsilon):
        """Îµ-greedyé€‰æ‹©åŠ¨ä½œ"""
        if random.random() < epsilon:
            return random.choice(ACTIONS)
        else:
            x, y, has_package = state
            return ACTIONS[np.argmax(self.q_table[x, y, has_package])]

    def update_q_table(self, state, action, reward, next_state):
        """æ›´æ–°Qè¡¨"""
        x, y, has_package = state
        action_idx = ACTIONS.index(action)

        # ä¸‹ä¸€çŠ¶æ€çš„æœ€å¤§Qå€¼
        next_x, next_y, next_has_package = next_state
        next_max_q = np.max(self.q_table[next_x, next_y, next_has_package])

        # Q-learningæ›´æ–°è§„åˆ™
        self.q_table[x, y, has_package, action_idx] = (1 - ALPHA) * self.q_table[x, y, has_package, action_idx] + \
                                                      ALPHA * (reward + GAMMA * next_max_q)


def train():
    env = CourierEnv()
    agent = QLearningAgent()

    for episode in range(EPISODES):
        state = env.reset()
        total_reward = 0

        while True:
            # é€‰æ‹©åŠ¨ä½œ
            action = agent.choose_action(state, EPSILON)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done = env.step(action)
            total_reward += reward

            # æ›´æ–°Qè¡¨
            agent.update_q_table(state, action, reward, next_state)

            if done:
                break
            state = next_state

        if (episode + 1) % 100 == 0:
            print(f"Episode {episode + 1}, Total Reward: {total_reward}")

    # è®­ç»ƒåå±•ç¤ºæœ€ä¼˜è·¯å¾„
    test_agent(agent)


def test_agent(agent):
    """æµ‹è¯•è®­ç»ƒåçš„æ™ºèƒ½ä½“"""
    env = CourierEnv()
    state = env.reset()
    path = [state[:2]]
    total_reward = 0

    print("\næœ€ä¼˜è·¯å¾„ï¼š")
    while True:
        action = agent.choose_action(state, epsilon=0)  # å…³é—­æ¢ç´¢
        next_state, reward, done = env.step(action)
        total_reward += reward
        path.append(next_state[:2])

        print(f"ä½ç½®: {state[:2]}, åŠ¨ä½œ: {action} -> æ–°ä½ç½®: {next_state[:2]}, å¥–åŠ±: {reward}")

        if done:
            print(f"æˆåŠŸé€è¾¾ï¼æ€»å¥–åŠ±: {total_reward}")
            break
        state = next_state

    # å¯è§†åŒ–è·¯å¾„
    grid = np.zeros((GRID_SIZE, GRID_SIZE), dtype=str)
    grid[:] = 'Â·'  # ç©ºåœ°
    grid[PACKAGE_POS] = 'ğŸ“¦'  # åŒ…è£¹
    grid[TARGET_POS] = 'ğŸ'  # ç›®æ ‡

    print("\nç½‘æ ¼è·¯å¾„ï¼š")
    for x, y in path:
        if (x, y) == START_POS:
            grid[x][y] = 'ğŸš¶'  # èµ·ç‚¹
        else:
            grid[x][y] = 'â˜…'  # è·¯å¾„ç‚¹

    for row in grid:
        print(' '.join(row))


if __name__ == "__main__":
    train()